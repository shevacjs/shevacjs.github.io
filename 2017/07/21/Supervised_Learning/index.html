<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Supervised learning ： Linear Regression - shevacjs home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="chenjiansen from CS229 Lecture notes by Andrew Ng  本文简单介绍下线性回归的基本思想和算法 基本定义设有 :   $$X = { x^{(1)},  x^{(2)}, … , x^{(m)} }$$$$Y = { y^{(1)},   y^{(2)}, … , y^{(m)} }$$ X表示输入集合，Y则代表对应的输出，则对于任意$x^{(i)}">
<meta name="keywords" content="ml">
<meta property="og:type" content="article">
<meta property="og:title" content="Supervised learning ： Linear Regression">
<meta property="og:url" content="http://shevacjs.com/2017/07/21/Supervised_Learning/index.html">
<meta property="og:site_name" content="shevacjs home">
<meta property="og:description" content="chenjiansen from CS229 Lecture notes by Andrew Ng  本文简单介绍下线性回归的基本思想和算法 基本定义设有 :   $$X = { x^{(1)},  x^{(2)}, … , x^{(m)} }$$$$Y = { y^{(1)},   y^{(2)}, … , y^{(m)} }$$ X表示输入集合，Y则代表对应的输出，则对于任意$x^{(i)}">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2017-07-21T11:16:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Supervised learning ： Linear Regression">
<meta name="twitter:description" content="chenjiansen from CS229 Lecture notes by Andrew Ng  本文简单介绍下线性回归的基本思想和算法 基本定义设有 :   $$X = { x^{(1)},  x^{(2)}, … , x^{(m)} }$$$$Y = { y^{(1)},   y^{(2)}, … , y^{(m)} }$$ X表示输入集合，Y则代表对应的输出，则对于任意$x^{(i)}">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo logo-text" href="/">Shevacjs</a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/atom.xml">Rss</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shevacjs.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-Supervised_Learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Supervised learning ： Linear Regression
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/07/21/Supervised_Learning/" class="article-date">
  <time datetime="2017-07-20T16:25:11.000Z" itemprop="datePublished">2017-07-21</time>
</a>
      
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>chenjiansen from CS229 Lecture notes by Andrew Ng</p>
</blockquote>
<p>本文简单介绍下线性回归的基本思想和算法</p>
<h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><p>设有 :  </p>
<p>$$X = { x^{(1)},  x^{(2)}, … , x^{(m)} }$$<br>$$Y = { y^{(1)},   y^{(2)}, … , y^{(m)} }$$</p>
<p>X表示输入集合，Y则代表对应的输出，则对于任意$x^{(i)}$, 有$x^{(i)} = {x_1, x_2,x_3, … x_n }$, 也就是我们假设这个数据集有<code>m</code>个元素，每个元素是<code>n</code>个维度的</p>
<p>同时，目标预测函数满足线性回归，则我们的假设函数可以写成: </p>
<p>$$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_nx_n = \sum_{i=0}^{n}\theta_ix_i = \theta^TX$$</p>
<p>同时定义我们的<code>cost function</code> ，我们可以把他定义为<code>差的平方和</code>(具体后面会解释为什么可以这样定义) : </p>
<p>$$ J(\theta) = \frac{1}{2} \sum_{i=0}^m(h_{(\theta)}(x^{(i)}) - y^{(i)})^2$$</p>
<p>则我们的问题可以简化为, 给定数据集合$X, Y$,  求让$J(\theta)$ 最小的$\theta$</p>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>最优化算法有非常多，这里面主要介绍两种最优化算法，包括<code>梯度下降法</code>和<code>最小二乘法</code>，下面分别说明 : </p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>考虑到上面定义的cost function，梯度下降法在这边的应用可以直接的表示为 : </p>
<p>$$ \theta_j : \theta_j - \alpha \frac{\partial}{\partial\theta} J(\theta)$$</p>
<p>其中$\alpha$表示下降的速度，$ j = { 0, 1, 2, … , n}$</p>
<p>下面我们来求解上面那个方程，简单起见，我们考虑只有一个样本的情况，如下: </p>
<p>$$ \frac{\partial}{\partial \theta} J(\theta) = \frac{1}{2} \ast 2 (h_{(\theta)}(x^{(i)}) - y^{(i)})  \ast \frac{\partial}{\partial\theta}{(h_{(\theta)}(x^{(i)} -y^{(i)})}$$</p>
<p>$$ = (h_{(\theta)}(x^{(i)}) - y^{(i)}) \ast \frac{\partial}{\partial\theta}{(\theta^TX -y^{(i)})} $$</p>
<p>$$ = (h_{(\theta)}(x^{(i)}) - y^{(i)}) \ast  x_j$$</p>
<p>也就是可以表示为:<br>$$ \theta_j =  \theta_j + \alpha \ast (h_{\theta}(x^{(i)}) - y^{(i)}) \ast  x_j $$</p>
<p>则一般在实际应用的时候，可以有两个办法：</p>
<p>第一个办法称为<code>batch gradient descent</code>,  大概算法如下 : </p>
<p>Repeat until convergence {<br>$$\theta_j = \theta_j + \alpha \sum_{i=0}^{m}( y^{(i)} - h_{\theta}(x^{(i)}) $$<br>[ for every j ]<br>}</p>
<p>第二个办法成为<code>incremental gradient descent</code> , 大概形式如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Loop &#123;</div><div class="line">	for i = 1 to m &#123;</div></pre></td></tr></table></figure></p>
<p>$        \theta_j = \theta_j + \alpha \ast ( y^{(i)} - h_{\theta}(x^{(i)})  $ ( for every j)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>可以看到: </p>
<ul>
<li><p>批量梯度下降: 最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下；每迭代一次需要的计算量约为<code>m*n^2</code>，如果m很大的话，性能上面会有比较大的压力</p>
</li>
<li><p>随机梯度下降 : 最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p>
</li>
</ul>
<h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><p>上面的优化问题，也可以用最小二乘法求解，依照输入的情况，我们定义如下几个变量 :</p>
<ul>
<li>矩阵 A :  m*n矩阵， 行向量 $x^{(j)}  $ 代表一个样本 </li>
<li>向量$\overrightarrow{\theta}$ : n*1 向量，也就是我们要求解的参数向量</li>
<li>向量 $\overrightarrow{y}$ :  m*1 向量，每个元素$y^{(i)}$, 对应着输入样本$x^{(i)}$的值</li>
</ul>
<p>则我们的问题可以转换为，求解向量$\overrightarrow{\theta}$, 使得 : </p>
<p>$$ A \overrightarrow {\theta} = \overrightarrow{y} $$</p>
<p>有最优解或者有解；无论矩阵A是否可逆那个，我们知道最小二乘的解为 (具体证明下面另述):</p>
<p>$$ \overrightarrow {\theta} = (A^TA)^{(-1)} A^T \overrightarrow{y} $$ </p>
<blockquote>
<p>个人理解: 缺点应该是整体的计算量会非常大</p>
</blockquote>
<h3 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h3><p>上面说到，我们使用<code>least-squares function</code> 来定义我们的cost function；下面简单说明，如此定义的合理性；</p>
<p>首先，对于预测结果和实际结果的关系，我们可以用如下式子说明 : </p>
<p>$$ y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$$</p>
<p>其中 $\epsilon^{(i)}$ 代表着我们的预估结果误差；</p>
<p>现在我们假设$\epsilon^{(i)}$ 满足正态分布，也就是$\epsilon^{(i)} \sim \aleph (0, \sigma^2)$ , 则有: </p>
<p>$$ p(y^{(i)} |  x^{(i)}, \theta ) = \frac{1}{\sqrt{2\pi}\sigma} exp (-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})$$</p>
<p>同样，现在我们面临的问题是，给定了集合${X, Y}$ , 需要预估一个合理的$\theta$,  所以我们定义似然函数如下: </p>
<p>$$ L(\theta) = L(\theta; X, \overrightarrow{y}) = p(\overrightarrow{y} | X; \theta)$$</p>
<p>如上，我们考虑到所有的$\epsilon^{(i)}$ 是独立的，所以有 ： </p>
<p>$$L(\theta) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sigma} exp (-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}) $$</p>
<p>现在就是要求这个函数的最大值, 我们求解<code>log likehood</code>的情况: </p>
<p>$$ l(\theta) = log L(\theta) = log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sigma} exp (-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})  $$</p>
<p>$$ = \sum_{i=0}^{m} log \frac{1}{\sqrt{2\pi}\sigma} exp (-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})$$</p>
<p>$$ = m  \ast log \frac{1}{\sqrt{2\pi}\sigma}  - \frac{1}{\sigma^2} \ast \frac{1}{2} \sum_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2$$</p>
<p>也就是令 $\frac{1}{2} \sum_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 $为最小值的时候其概率最大； 正好对应着我们定义的<code>cost function</code></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://www.cnblogs.com/maybe2030/p/4751804.html" target="_blank" rel="external">常见的几种最优化方法</a> : 几个常见的优化算法，说的非常详细</li>
</ul>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ml/">ml</a></li></ul>

      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/07/21/bayes/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          关于贝叶斯分类
        
      </div>
    </a>
  
  
    <a href="/2017/07/21/svm/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">About SVM&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>






<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>


</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 shevacjs&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    
<script>
  var disqus_shortname = 'shevacjs-com';
  
  var disqus_url = 'http://shevacjs.com/2017/07/21/Supervised_Learning/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>